--- 
layout: post
title: Different approach to scaling a Webcrawler
published: true
---
<p>In a recent project, we had to read users' twitter timelines, profiles and friends information for about 10 million users. The standard recipe is simple. Implement the script to fetch from twitter, Write to &lt;insert your favorite&gt; Nosql database and run queries over it.&nbsp; Very simple. Right?</p>
<p>The only key issue would be how to scale Twitter Reads, The write part should be simple because we are using a Nosql database( a myth i will try and explode in some other post) in this case Couchdb. Using some ingenious techniques we scaled the twitter reads, employing a lot of threads and a few machines.</p>
<p>But when it was implemented it became a real dog. In over a few weeks it could fetch about 30,000 users' timelines. Which is almost about retrieving 1 User Timeline per minute, even with 50 concurrent threads running. All that fancy hardware and funky programming down the drain. We needed this data fast and very quickly</p>
<p>In a weekend of madness we rewrote the crawlers as a single ruby process that fetches from twitter and writes into a file system. We run a whole bunch of them together at the same time to achieve scale. We use <a href="http://redis.io/">Redis</a> to maintain a simple queue for these crawlers to figure out which user information is needed. With these changes we were fetching about 150,000 user timelines/info in a day.</p>
<p>We also wrote a 20 line <a href="http://nodejs.org/">node.js</a> application to provide a simple HTTP API over this file system data for the internal applications' usage.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&lt;Diagram here.&gt;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>Filesystems are fast doing reads and writes, As in it's still order of a few milliseconds to read from a specific file and write to a specific file. (it's almost irrelevant how many files are in the folder) On the other hand things like how many users exist in the filesystem and does the user exist somewhere in the file system is really slow.We use the <a href="http://redis.io/">Redis</a> to track such information as well.</p>
<p>There were a couple of issues with it. We used ext3 file system initially and ran into it's '32K max files in a folder' limit, we switched to using ext4 volumes instead and avoid the whole issue all together.</p>
<p>We were also maxing out the disk on IO (on EC2 it was already slow) as well as capacity. The only way to add more workers was to split the writes to different disks spindles (or whatever they use on EC2). So we sharded our file system writes.</p>
<p>As anybody who has used sharding databases knows, it is important choose a key correctly so as not to create hotspot disks. In our case we use twitter names as the key. But it was intuitive that since name follow <a href="http://en.wikipedia.org/wiki/Zipf%27s_law">zipfian distribution</a>&nbsp; there would be hotspots. But after experimenting with username SHA(which is what a lot of databases use) we fell back to using just&nbsp; the name to shard. We followed this simple scheme and it worked very well for us.</p>
<p><em><span style="font-size: small; color: #888888;">&nbsp; {</span></em><br /> <em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "^[a-l]" : "/vol_1/data/twitter/",</span></em><br /> <em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "^[l-z0-9_]" : "/vol_2/data/twitter/"&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; </span></em><br /> <em><span style="font-size: small; color: #888888;">&nbsp; }</span></em></p>
<p>All users with name starting A-L go to shard 1</p>
<p>All users with name starting L-Z0-9_ go to shard 2</p>
<p>&nbsp;</p>
<p>Another issue to deal with when sharding is how to reshard, when new volumes are added or old volumes are removed. This was a real problem for us as well because at the rate we started off, we expected to add a few more shards in a few weeks.</p>
<p>A lot of distributed databases use <a href="http://en.wikipedia.org/wiki/Consistent_hashing">Consistent Hashing</a> to reshard. But that still means we will have to remap files to the new shard, even small number of remapping means significant network traffic and writing more complicated code to manage this. We instead chose to not remap anything. Instead we maintained Shard histories. Something like this.</p>
<p><em><span style="font-size: small; color: #888888;">[{</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp; "^[a-d]" : "/vol_1/data/twitter/",</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp; "^[l-r]" : "/vol_2/data/twitter/",</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp; "^[e-k]" : "/vol_3/data/twitter/",</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp; "^[s-z0-9_]" : "/vol_4/data/twitter/"</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp; }, </span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp; {</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "^[a-l]" : "/vol_1/data/twitter/",</span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "^[l-z0-9_]" : "/vol_2/data/twitter/"&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; </span></em><br /><em><span style="font-size: small; color: #888888;">&nbsp; }]</span></em></p>
<p>&nbsp;</p>
<p>What that means is we would write data to the the first shard that matches, but would progressively scan the previous shard histories to find if the user file exists before giving up(and returning a 404). At the expense of small delay at read, we get free resharding.</p>
<p>One of the biggest revelation from this was &nbsp; filesystem is a wonderful but underrated and underused distributed database. It has a long history of optimisation and scales reads and writes very well. It also has a ton of tools to monitor, manage and handle errors as well that most people are familier with.</p>
<p>&nbsp;</p>
